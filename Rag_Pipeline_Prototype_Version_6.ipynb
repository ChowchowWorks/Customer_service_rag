{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPkPcr3HjdRaWY3nsDrTdKV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChowchowWorks/Customer_service_rag/blob/main/Rag_Pipeline_Prototype_Version_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: Set up the environment"
      ],
      "metadata": {
        "id": "UGBH1vAROFdc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WHIGUs5yYcy",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "import os\n",
        "\n",
        "!pip install \"transformers==4.49.0\"\n",
        "!pip install -U langchain langchain-community langchainhub langchain-huggingface tiktoken\n",
        "!pip install -U chromadb sentence-transformers\n",
        "!pip install -U pypdf\n",
        "!pip install -U ragatouille\n",
        "\n",
        "\n",
        "os.environ['LANGCHAIN_API_KEY'] = \"API_KEY\"\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = \"API_KEY\"\n",
        "os.environ['USER_AGENT'] = 'MyColabApp/1.0 (Python/3.9; GoogleColab)'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2: Loading Documents"
      ],
      "metadata": {
        "id": "eLPP5KJdOKwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "\n",
        "# For the purpose of testing, this version uses a pdf loader\n",
        "loader = PyPDFDirectoryLoader(\"/content/RAG tester\")\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "UXLdyVVqOj2f"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3: Indexing"
      ],
      "metadata": {
        "id": "Poh05nx-O9Mg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Indexing algorithm"
      ],
      "metadata": {
        "id": "9_k2WwE_P87j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ragatouille import RAGPretrainedModel\n",
        "!pip install 'faiss-gpu'\n",
        "\n",
        "# Set up the indexing model\n",
        "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "\n",
        "# Index the documents\n",
        "RAG.index(collection = [doc.page_content for doc in documents], index_name = \"Behavioral\", max_document_length= 180, split_documents= True )"
      ],
      "metadata": {
        "collapsed": true,
        "id": "30_WpfbAO8tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Set up the retriever"
      ],
      "metadata": {
        "id": "cTgEsrx8QAJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = RAG.as_langchain_retriever(index_name = \"Behavioral\", k = 5)"
      ],
      "metadata": {
        "id": "u45Fic6xQCx1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4: Setting up the Generator"
      ],
      "metadata": {
        "id": "xr5vFSdEzstj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "client = InferenceClient(model=\"mistralai/Mistral-7B-Instruct-v0.3\", token = os.environ['HUGGINGFACEHUB_API_TOKEN'])\n",
        "\n",
        "from langchain_core.runnables import Runnable\n",
        "\n",
        "class HuggingFaceChatRunnable(Runnable):\n",
        "    def __init__(self, client, prompt_template, temperature, max_tokens):\n",
        "        self.client = client\n",
        "        self.prompt_template = prompt_template\n",
        "        self.temperature = temperature\n",
        "        self.max_tokens = max_tokens\n",
        "\n",
        "    def invoke(self, inputs: dict, config: dict = None) -> str:\n",
        "        prompt_str = self.prompt_template.format(**inputs)\n",
        "\n",
        "        response = self.client.chat_completion(\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt_str}\n",
        "            ],\n",
        "            temperature=self.temperature,\n",
        "            max_tokens=self.max_tokens\n",
        "        )\n",
        "        return response.choices[0].message[\"content\"]"
      ],
      "metadata": {
        "id": "s3VAUxZvzvbm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 5: Routing Prompts"
      ],
      "metadata": {
        "id": "kkZ_OGC_zyya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Fewshot examples of intent detection"
      ],
      "metadata": {
        "id": "Wi0wQZi5z4xI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.prompts import FewShotChatMessagePromptTemplate\n",
        "\n",
        "# Some fewshot examples\n",
        "examples =[\n",
        "    {\n",
        "        \"input\": \"What is creatine?\",\n",
        "        \"output\": \"DEFINE\",\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Why do athletes take protein after workouts?\",\n",
        "        \"output\": \"EXPLAIN\",\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"How do I calculate my calorie needs?\",\n",
        "        \"output\": \"PROCEDURE\",\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Should I take whey or casein protein?\",\n",
        "        \"output\": \"COMPARISON\",\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"What is the best way to embark on my weight loss journey?\",\n",
        "        \"output\": \"ADVICE\",\n",
        "    },\n",
        "]\n",
        "\n",
        "\n",
        "example_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"ai\", \"{output}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "few_shot_examples = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples,\n",
        ")"
      ],
      "metadata": {
        "id": "MAlN4QaAz20Z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Set up the intent detection prompt"
      ],
      "metadata": {
        "id": "AmZJAoUIz_Io"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "intent_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"You are an intent classifier for the field of interest in the query.\n",
        "Given a question, classify it into one of the following intents:\n",
        "- DEFINE: Asking for a definition or description\n",
        "- EXPLAIN: Asking for reasoning or why something is the case\n",
        "- PROCEDURE: Asking for how-to or steps\n",
        "- ADVICE: Asking for personalized or practical suggestions\n",
        "- COMPARISON: Asking to compare options\n",
        "- GENERAL: Anything else\n",
        "Return only the intent, nothing else.\n",
        "Here are a few examples:\"\"\",\n",
        "        ),\n",
        "        # few shot examples\n",
        "        few_shot_examples,\n",
        "        # New question\n",
        "        (\"user\", \"{question}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "wsrOhMVp0DTv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) Set up routing"
      ],
      "metadata": {
        "id": "umxU_wse0IHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "router = HuggingFaceChatRunnable(client, intent_prompt, 0.0, 10)"
      ],
      "metadata": {
        "id": "7tzX4C210L7E"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 6: Step-back Translation"
      ],
      "metadata": {
        "id": "1BfK7P-S0O6-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Step-back prompts"
      ],
      "metadata": {
        "id": "2A5uQmfI0Xap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This are examples that shows the LLM what it is achieving through stepback\n",
        "\n",
        "examples = [\n",
        "    {\n",
        "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
        "        \"output\": \"what can the members of The Police do?\",\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
        "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
        "    },\n",
        "]\n",
        "\n",
        "# Now translate this into an example_prompt\n",
        "example_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"ai\", \"{output}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "few_shot = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples,\n",
        ")\n",
        "\n",
        "step_back_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
        "        ),\n",
        "        # Few shot examples\n",
        "        few_shot,\n",
        "        # New question\n",
        "        (\"user\", \"Intent: {intent}\\nQuestion: {question}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "gJIXMna00WQt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Set-up stepback"
      ],
      "metadata": {
        "id": "2AAmvxbw0bxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stepback = HuggingFaceChatRunnable(client, step_back_prompt, 0.0, 1024)"
      ],
      "metadata": {
        "id": "_AZzsWku0iCN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 7: Routing Chains"
      ],
      "metadata": {
        "id": "NFXjseF50lee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Manual Chain prompts"
      ],
      "metadata": {
        "id": "k99DBZxK0qsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define prompt\n",
        "defineprompt = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Answer the question using only the information in the provided context blocks.\n",
        "You are responding to a query with the intent: DEFINE.\n",
        "Your answer should be:\n",
        "- Use the retrieved context as your only source of truth\n",
        "- Do not rely on external or prior knowledge, even if you think it’s correct\n",
        "- If the context does not contain enough information, say \"The information is not available in the context provided.\"\n",
        "- Comprehensive, but concise (1–3 sentences max)\n",
        "- Factually correct and aligned with the provided context\n",
        "- Free of speculation, advice, or subjective judgment\n",
        "- Focused only on essential information—no unnecessary background or examples unless they resolve ambiguity\n",
        "- Adjusted for multiple meanings if applicable\n",
        "- Written in terminology appropriate to the user's domain or field\n",
        "\n",
        "\n",
        "# {normal_context}\n",
        "# {step_back_context}\n",
        "\n",
        "# Original Question: {question}\n",
        "# Answer:\"\"\"\n",
        "\n",
        "#explain prompt\n",
        "explainprompt = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Answer the question using only the information in the provided context blocks.\n",
        "You are responding to a query with the intent: EXPLAIN.\n",
        "Your answer should be:\n",
        "- Use the retrieved context as your only source of truth\n",
        "- Do not rely on external or prior knowledge, even if you think it’s correct\n",
        "- If the context does not contain enough information, say \"The information is not available in the context provided.\"\n",
        "- Clear and logically structured\n",
        "- Focused on cause, reasoning, background, or significance\n",
        "- Factually correct and aligned with the provided context\n",
        "- Neutral in tone—avoid persuasion, speculation, or personal opinions\n",
        "- Examples are welcome from the context provided, if it helps to improve understanding.\n",
        "\n",
        "# {normal_context}\n",
        "# {step_back_context}\n",
        "\n",
        "# Original Question: {question}\n",
        "# Answer:\"\"\"\n",
        "\n",
        "#procedure prompt\n",
        "procedureprompt = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Answer the question using only the information in the provided context blocks.\n",
        "You are responding to a query with the intent: PROCEDURE.\n",
        "Your answer should be:\n",
        "- Use the retrieved context as your only source of truth\n",
        "- Do not rely on external or prior knowledge, even if you think it’s correct\n",
        "- If the context does not contain enough information, say \"The information is not available in the context provided.\"\n",
        "- Structured as a clear, ordered list of steps (e.g., 1, 2, 3...)\n",
        "- Focused on how-to instructions or best-practice sequences\n",
        "- Specific, practical, and applicable to the user’s likely context\n",
        "- Factually accurate and based on reliable knowledge\n",
        "- Aligned with the provided context; ignore context if irrelevant\n",
        "\n",
        "# {normal_context}\n",
        "# {step_back_context}\n",
        "\n",
        "# Original Question: {question}\n",
        "# Answer:\n",
        "1.\n",
        "2.\n",
        "3.\"\"\"\n",
        "\n",
        "#advice prompt\n",
        "adviceprompt = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Answer the question using only the information in the provided context blocks.\n",
        "You are responding to a query with the intent: ADVICE.\n",
        "Your answer should be:\n",
        "- Use the retrieved context as your only source of truth\n",
        "- Do not rely on external or prior knowledge, even if you think it’s correct\n",
        "- If the context does not contain enough information, say \"The information is not available in the context provided.\"\n",
        "- Actionable and practical, tailored to a general user (not personalized)\n",
        "- Fact-based, but sensitive to nuance, caution, or best practices\n",
        "- Free from subjective judgment or emotional language\n",
        "- Respectful of varying conditions or assumptions\n",
        "- Aligned with the provided context; if not relevant, ignore the context\n",
        "\n",
        "# {normal_context}\n",
        "# {step_back_context}\n",
        "\n",
        "# Original Question: {question}\n",
        "# Answer:\"\"\"\n",
        "\n",
        "#comparison\n",
        "comparisonprompt = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Answer the question using only the information in the provided context blocks.\n",
        "You are responding to a query with the intent: COMPARISON.\n",
        "Your answer should be:\n",
        "- Use the retrieved context as your only source of truth\n",
        "- Do not rely on external or prior knowledge, even if you think it’s correct\n",
        "- If the context does not contain enough information, say \"The information is not available in the context provided.\"\n",
        "- A neutral, side-by-side analysis of options or alternatives\n",
        "- Factually grounded—avoid personal recommendations unless one option is clearly superior based on evidence\n",
        "- Clearly structured with bullet points or short paragraphs\n",
        "- Helpful in illustrating pros and cons, similarities, and differences\n",
        "- Consistent with the context provided; ignore it if irrelevant\n",
        "\n",
        "# {normal_context}\n",
        "# {step_back_context}\n",
        "\n",
        "# Original Question: {question}\n",
        "# Answer:\n",
        "Option A:\n",
        "Option B: \"\"\"\n",
        "\n",
        "#general prompt\n",
        "generalprompt = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Answer the question using only the information in the provided context blocks.\n",
        "You are responding to a query with the intent: GENERAL.\n",
        "Your answer should be:\n",
        "- Use the retrieved context as your only source of truth\n",
        "- Do not rely on external or prior knowledge, even if you think it’s correct\n",
        "- If the context does not contain enough information, say \"The information is not available in the context provided.\"\n",
        "- Informative and contextually aware\n",
        "- Concise but flexible in length (aim for clarity)\n",
        "- Objective and based on verifiable information\n",
        "- Avoid speculation or personal opinion\n",
        "- Aligned with the provided context if relevant\n",
        "\n",
        "# {normal_context}\n",
        "# {step_back_context}\n",
        "\n",
        "# Original Question: {question}\n",
        "# Answer:\"\"\"\n"
      ],
      "metadata": {
        "id": "Ue7YeoCb0lMd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Load prompt into router"
      ],
      "metadata": {
        "id": "v2tDIEyP0xhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "intent_router = RunnableLambda(lambda x: {\n",
        "    \"DEFINE\": HuggingFaceChatRunnable(client, defineprompt, 0.0, 1024),\n",
        "    \"EXPLAIN\": HuggingFaceChatRunnable(client, explainprompt, 0.0, 1024),\n",
        "    \"PROCEDURE\": HuggingFaceChatRunnable(client, procedureprompt, 0.0, 1024),\n",
        "    \"ADVICE\": HuggingFaceChatRunnable(client, adviceprompt, 0.0, 1024),\n",
        "    \"COMPARISON\": HuggingFaceChatRunnable(client, comparisonprompt, 0.0, 1024),\n",
        "    \"GENERAL\": HuggingFaceChatRunnable(client, generalprompt, 0.0, 1024),\n",
        "}[x[\"intent\"].strip()]\n",
        ")"
      ],
      "metadata": {
        "id": "ijJm7b6k078G"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) Build the RAG chain"
      ],
      "metadata": {
        "id": "t_LRMm4K1A_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableMap, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "chain = (\n",
        "    RunnableMap({\n",
        "        \"question\": lambda x: x[\"question\"],\n",
        "        \"step_back_question\": lambda x: x[\"question\"],\n",
        "        \"intent\": lambda x: router.invoke({\"question\": x[\"question\"]})\n",
        "    })\n",
        "    | RunnableLambda(lambda x: {\n",
        "        \"normal_context\": retriever.invoke(x[\"question\"]),\n",
        "        \"step_back_q\": stepback.invoke({\"intent\" : x[\"intent\"],\"question\": x[\"step_back_question\"]}),\n",
        "        \"question\": x[\"question\"],\n",
        "        \"intent\": next(iter(x[\"intent\"])) if isinstance(x[\"intent\"], set) else x[\"intent\"]\n",
        "    })\n",
        "    | RunnableLambda(lambda x: {\n",
        "        \"step_back_context\": retriever.invoke(x[\"step_back_q\"]),\n",
        "        \"normal_context\": x[\"normal_context\"],\n",
        "        \"question\": x[\"question\"],\n",
        "        \"intent\": x[\"intent\"]\n",
        "    })\n",
        "    | intent_router\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "lQTZToWx1HVC"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 8: Testing the RAG Pipeline"
      ],
      "metadata": {
        "id": "72JqNV2x1OBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = input(\"Ask me anything! \\n\")\n",
        "\n",
        "# Generate the Response\n",
        "response = chain.invoke({\"question\": question})\n",
        "print(response)"
      ],
      "metadata": {
        "id": "obihAPjB1Rv7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}