{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIbW7XfvH4zV0m+byUkV4v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChowchowWorks/Customer_service_rag/blob/main/Rag_Pipeline_Prototype_Version_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: Setting up the Environment"
      ],
      "metadata": {
        "id": "A7AVvxcDEHR5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Stv9ptAUebzo",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "!pip install langchain_community langchain chromadb transformers sentence-transformers\n",
        "!pip install -U langchain-huggingface\n",
        "\n",
        "!pip install pypdf\n",
        "!pip install langchain langchain-community langchainhub tiktoken\n",
        "!pip install -U sentence-transformers\n",
        "\n",
        "\n",
        "os.environ['LANGCHAIN_API_KEY'] = \"API_TOKEN\"\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = \"API_TOKEN\"\n",
        "os.environ['USER_AGENT'] = 'MyColabApp/1.0 (Python/3.9; GoogleColab)'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2: Loading Documents"
      ],
      "metadata": {
        "id": "vs9g9P74EMDs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Loading documents into a global variable"
      ],
      "metadata": {
        "id": "vNEPYeNBETnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "\n",
        "# For the purpose of testing, this version uses a pdf loader\n",
        "loader = PyPDFDirectoryLoader(\"/content/RAG_tester\")\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "oYvv9D92EW3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Text Splitting"
      ],
      "metadata": {
        "id": "IPcue1yIEZ52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 100) # These are hyperparameters, can attempt tuning this using bayesian optimisation in the future\n",
        "texts = splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "0DicmR3pEcbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3: Embedding"
      ],
      "metadata": {
        "id": "5FGLGEs5Gdcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "MkSZE_YcHHWr",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4: Generator"
      ],
      "metadata": {
        "id": "l4MBvPW7tjZx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Client and Class"
      ],
      "metadata": {
        "id": "EVnWJYsxtpto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "client = InferenceClient(model=\"mistralai/Mistral-7B-Instruct-v0.3\", token = os.environ['HUGGINGFACEHUB_API_TOKEN'])\n",
        "\n",
        "from langchain_core.runnables import Runnable\n",
        "\n",
        "class HuggingFaceChatRunnable(Runnable):\n",
        "    def __init__(self, client, prompt_template, temperature, max_tokens):\n",
        "        self.client = client\n",
        "        self.prompt_template = prompt_template\n",
        "        self.temperature = temperature\n",
        "        self.max_tokens = max_tokens\n",
        "\n",
        "    def invoke(self, inputs: dict, config: dict = None) -> str:\n",
        "        prompt_str = self.prompt_template.format(**inputs)\n",
        "\n",
        "        response = self.client.chat_completion(\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt_str}\n",
        "            ],\n",
        "            temperature=self.temperature,\n",
        "            max_tokens=self.max_tokens\n",
        "        )\n",
        "        return response.choices[0].message[\"content\"]"
      ],
      "metadata": {
        "id": "DQhuyhshto4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 5: RAPTOR (Gaussian Mixture)"
      ],
      "metadata": {
        "id": "EerNJHxYuIT3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Clustering Structure"
      ],
      "metadata": {
        "id": "xQAqhP0tuo6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List, Optional, Tuple\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import umap\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "seed = 1000\n",
        "\n",
        "# -- Credit for code below belongs to parthsarthi03, llama_index and lang_chain -- #\n",
        "\n",
        "def global_cluster_embeddings(\n",
        "    embeddings: np.ndarray,\n",
        "    dim: int,\n",
        "    n_neighbors: Optional[int] = None,\n",
        "    metric: str = \"cosine\",\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Perform global dimensionality reduction on the embeddings using UMAP.\n",
        "\n",
        "    Parameters:\n",
        "    - embeddings: The input embeddings as a numpy array.\n",
        "    - dim: The target dimensionality for the reduced space.\n",
        "    - n_neighbors: Optional; the number of neighbors to consider for each point.\n",
        "                   If not provided, it defaults to the square root of the number of embeddings.\n",
        "    - metric: The distance metric to use for UMAP.\n",
        "\n",
        "    Returns:\n",
        "    - A numpy array of the embeddings reduced to the specified dimensionality.\n",
        "    \"\"\"\n",
        "    if n_neighbors is None:\n",
        "        n_neighbors = int((len(embeddings) - 1) ** 0.5)\n",
        "    return umap.UMAP(\n",
        "        n_neighbors=n_neighbors, n_components=dim, metric=metric\n",
        "    ).fit_transform(embeddings)\n",
        "\n",
        "def local_cluster_embeddings(\n",
        "    embeddings: np.ndarray, dim: int, num_neighbors: int = 10, metric: str = \"cosine\"\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Perform local dimensionality reduction on the embeddings using UMAP, typically after global clustering.\n",
        "\n",
        "    Parameters:\n",
        "    - embeddings: The input embeddings as a numpy array.\n",
        "    - dim: The target dimensionality for the reduced space.\n",
        "    - num_neighbors: The number of neighbors to consider for each point.\n",
        "    - metric: The distance metric to use for UMAP.\n",
        "\n",
        "    Returns:\n",
        "    - A numpy array of the embeddings reduced to the specified dimensionality.\n",
        "    \"\"\"\n",
        "    return umap.UMAP(\n",
        "        n_neighbors=num_neighbors, n_components=dim, metric=metric\n",
        "    ).fit_transform(embeddings)\n",
        "\n",
        "def get_optimal_clusters(\n",
        "    embeddings: np.ndarray, max_clusters: int = 50, random_state: int = seed\n",
        ") -> int:\n",
        "    \"\"\"\n",
        "    Determine the optimal number of clusters using the Bayesian Information Criterion (BIC) with a Gaussian Mixture Model.\n",
        "\n",
        "    Parameters:\n",
        "    - embeddings: The input embeddings as a numpy array.\n",
        "    - max_clusters: The maximum number of clusters to consider.\n",
        "    - random_state: Seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "    - An integer representing the optimal number of clusters found.\n",
        "    \"\"\"\n",
        "    max_clusters = min(max_clusters, len(embeddings))\n",
        "    n_clusters = np.arange(1, max_clusters)\n",
        "    bics = []\n",
        "    for n in n_clusters:\n",
        "        gm = GaussianMixture(n_components=n, random_state=random_state)\n",
        "        gm.fit(embeddings)\n",
        "        bics.append(gm.bic(embeddings))\n",
        "    return n_clusters[np.argmin(bics)]\n",
        "\n",
        "def GMM_cluster(embeddings: np.ndarray, threshold: float, random_state: int = 0):\n",
        "    \"\"\"\n",
        "    Cluster embeddings using a Gaussian Mixture Model (GMM) based on a probability threshold.\n",
        "\n",
        "    Parameters:\n",
        "    - embeddings: The input embeddings as a numpy array.\n",
        "    - threshold: The probability threshold for assigning an embedding to a cluster.\n",
        "    - random_state: Seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "    - A tuple containing the cluster labels and the number of clusters determined.\n",
        "    \"\"\"\n",
        "    n_clusters = get_optimal_clusters(embeddings)\n",
        "    gm = GaussianMixture(n_components=n_clusters, random_state=random_state)\n",
        "    gm.fit(embeddings)\n",
        "    probs = gm.predict_proba(embeddings)\n",
        "    labels = [np.where(prob > threshold)[0] for prob in probs]\n",
        "    return labels, n_clusters\n",
        "\n",
        "\n",
        "def perform_clustering(\n",
        "    embeddings: np.ndarray,\n",
        "    dim: int,\n",
        "    threshold: float,\n",
        ") -> List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Perform clustering on the embeddings by first reducing their dimensionality globally, then clustering\n",
        "    using a Gaussian Mixture Model, and finally performing local clustering within each global cluster.\n",
        "\n",
        "    Parameters:\n",
        "    - embeddings: The input embeddings as a numpy array.\n",
        "    - dim: The target dimensionality for UMAP reduction.\n",
        "    - threshold: The probability threshold for assigning an embedding to a cluster in GMM.\n",
        "\n",
        "    Returns:\n",
        "    - A list of numpy arrays, where each array contains the cluster IDs for each embedding.\n",
        "    \"\"\"\n",
        "    if len(embeddings) <= dim + 1:\n",
        "        # Avoid clustering when there's insufficient data\n",
        "        return [np.array([0]) for _ in range(len(embeddings))]\n",
        "\n",
        "    # Global dimensionality reduction\n",
        "    reduced_embeddings_global = global_cluster_embeddings(embeddings, dim)\n",
        "    # Global clustering\n",
        "    global_clusters, n_global_clusters = GMM_cluster(\n",
        "        reduced_embeddings_global, threshold\n",
        "    )\n",
        "\n",
        "    all_local_clusters = [np.array([]) for _ in range(len(embeddings))]\n",
        "    total_clusters = 0\n",
        "\n",
        "    # Iterate through each global cluster to perform local clustering\n",
        "    for i in range(n_global_clusters):\n",
        "        # Extract embeddings belonging to the current global cluster\n",
        "        global_cluster_embeddings_ = embeddings[\n",
        "            np.array([i in gc for gc in global_clusters])\n",
        "        ]\n",
        "\n",
        "        if len(global_cluster_embeddings_) == 0:\n",
        "            continue\n",
        "        if len(global_cluster_embeddings_) <= dim + 1:\n",
        "            # Handle small clusters with direct assignment\n",
        "            local_clusters = [np.array([0]) for _ in global_cluster_embeddings_]\n",
        "            n_local_clusters = 1\n",
        "        else:\n",
        "            # Local dimensionality reduction and clustering\n",
        "            reduced_embeddings_local = local_cluster_embeddings(\n",
        "                global_cluster_embeddings_, dim\n",
        "            )\n",
        "            local_clusters, n_local_clusters = GMM_cluster(\n",
        "                reduced_embeddings_local, threshold\n",
        "            )\n",
        "\n",
        "        # Assign local cluster IDs, adjusting for total clusters already processed\n",
        "        for j in range(n_local_clusters):\n",
        "            local_cluster_embeddings_ = global_cluster_embeddings_[\n",
        "                np.array([j in lc for lc in local_clusters])\n",
        "            ]\n",
        "            indices = np.where(\n",
        "                (embeddings == local_cluster_embeddings_[:, None]).all(-1)\n",
        "            )[1]\n",
        "            for idx in indices:\n",
        "                all_local_clusters[idx] = np.append(\n",
        "                    all_local_clusters[idx], j + total_clusters\n",
        "                )\n",
        "\n",
        "        total_clusters += n_local_clusters\n",
        "\n",
        "    return all_local_clusters"
      ],
      "metadata": {
        "id": "9jenXT1wuIA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Helper functions for recursive clustering"
      ],
      "metadata": {
        "id": "vYK7vlh9wKQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "# -- codes below are sourced and inspired by langchain -- #\n",
        "\n",
        "def embedding_to_np(texts): # convert text embeddings into a numpy array\n",
        "  text_embeddings = embeddings.embed_documents([t.page_content for t in texts])\n",
        "  return np.array(text_embeddings)\n",
        "\n",
        "def embed_cluster_texts(texts): # perform clustering on the numpy array\n",
        "  np_embeddings = embedding_to_np(texts)\n",
        "  cluster_labels = perform_clustering(np_embeddings, 10, 0.1)\n",
        "  df = pd.DataFrame()\n",
        "  df[\"text\"] = texts\n",
        "  df[\"embd\"] = list(np_embeddings)\n",
        "  df[\"cluster\"] = cluster_labels\n",
        "  return df\n",
        "\n",
        "def format_texts(df): # reformat the embedded text\n",
        "  unique = df[\"text\"].tolist()\n",
        "  # Extract page_content from Document objects before joining\n",
        "  unique_str = [doc.page_content for doc in unique]\n",
        "  return \"--- --- \\n --- --- \".join(unique_str)\n",
        "\n",
        "def embed_cluster_summarised_text(texts, level) -> Tuple[pd.DataFrame, pd.DataFrame]: # embeds, clusters and summarises a list of texts\n",
        "  df_clusters = embed_cluster_texts(texts)\n",
        "  expanded_list = []\n",
        "  for index, row in df_clusters.iterrows(): # expand dataframe entries to document-cluster pairings for straightforward processing\n",
        "        for cluster in row[\"cluster\"]:\n",
        "            expanded_list.append(\n",
        "                {\"text\": row[\"text\"], \"embd\": row[\"embd\"], \"cluster\": cluster}\n",
        "            )\n",
        "  expanded_df = pd.DataFrame(expanded_list)\n",
        "  all_clusters = expanded_df[\"cluster\"].unique()\n",
        "  print(f\"--Generated {len(all_clusters)} clusters--\")\n",
        "  # Summary\n",
        "  template = \"\"\"Here is a subset of books focussed on behavioral economics.\n",
        "\n",
        "    Behavioral Economics is a branch of economics that incorporates insights from psychology and emphasizes the importance of supposed irrelevant factors in economic analysis.\n",
        "\n",
        "    Give a detailed summary of the documentation provided.\n",
        "\n",
        "    Documentation:\n",
        "    {context}\n",
        "    \"\"\"\n",
        "\n",
        "  prompt = ChatPromptTemplate.from_template(template)\n",
        "  summariser = HuggingFaceChatRunnable(client, prompt, 0.0, 1024)\n",
        "\n",
        "  summaries = [] # Format text within each cluster for summarization\n",
        "  for i in all_clusters:\n",
        "        df_cluster = expanded_df[expanded_df[\"cluster\"] == i]\n",
        "        formatted_txt = format_texts(df_cluster)\n",
        "        summaries.append(summariser.invoke({\"context\": formatted_txt}))\n",
        "  df_summary = pd.DataFrame(\n",
        "        {\n",
        "            \"summaries\": summaries,\n",
        "            \"level\": [level] * len(summaries),\n",
        "            \"cluster\": list(all_clusters),\n",
        "        }\n",
        "    )\n",
        "  return df_clusters, df_summary\n",
        "\n",
        "def recursive_embed_cluster_summarize(\n",
        "    texts: List[str], level: int = 1, n_levels: int = 3\n",
        ") -> Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:\n",
        "  results = {} # dictionary to store results\n",
        "  df_clusters, df_summary = embed_cluster_summarised_text(texts, level)\n",
        "  results[level] = (df_clusters, df_summary)\n",
        "\n",
        "  # determine if further recursion and meaningful\n",
        "  unique_clusters = df_clusters[\"cluster\"].nunique()\n",
        "  if level < n_levels and unique_clusters > 1:\n",
        "    new_texts = df_summary[\"summaries\"].tolist() # Use summaries as the input texts for the next level of recursion\n",
        "    next_level_results = recursive_embed_cluster_summarize(\n",
        "        new_texts, level + 1, n_levels\n",
        "    )\n",
        "    results.update(next_level_results)\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "psaiORNhwNWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) Performing recursive cluster"
      ],
      "metadata": {
        "id": "rpVF7whnDov4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "leaf_texts = texts\n",
        "results = recursive_embed_cluster_summarize(leaf_texts, level = 1, n_levels= 3)"
      ],
      "metadata": {
        "id": "GJ0Ol1hrDr8r",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(d) Set up the retriever"
      ],
      "metadata": {
        "id": "DGFDJQqhw3NL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = Chroma.from_documents(results, embeddings)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})"
      ],
      "metadata": {
        "id": "F7sMf2x4w5oD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 6: Routing Prompts"
      ],
      "metadata": {
        "id": "WMEvcFNIvl0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Produce some fewshot examples to incorporate into routing prompt"
      ],
      "metadata": {
        "id": "AbNfhNI2vtn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.prompts import FewShotChatMessagePromptTemplate\n",
        "\n",
        "# Some fewshot examples\n",
        "examples =[\n",
        "    {\n",
        "        \"input\": \"What is creatine?\",\n",
        "        \"output\": \"DEFINE\",\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Why do athletes take protein after workouts?\",\n",
        "        \"output\": \"EXPLAIN\",\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"How do I calculate my calorie needs?\",\n",
        "        \"output\": \"PROCEDURE\",\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Should I take whey or casein protein?\",\n",
        "        \"output\": \"COMPARISON\",\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"What is the best way to embark on my weight loss journey?\",\n",
        "        \"output\": \"ADVICE\",\n",
        "    },\n",
        "]\n",
        "\n",
        "\n",
        "example_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"ai\", \"{output}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "few_shot_examples = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples,\n",
        ")\n"
      ],
      "metadata": {
        "id": "bP21mGAUvlaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Setting up the intent prompt"
      ],
      "metadata": {
        "id": "FAEjhrMkvzXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "intent_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"You are an intent classifier for the field of interest in the query.\n",
        "Given a question, classify it into one of the following intents:\n",
        "- DEFINE: Asking for a definition or description\n",
        "- EXPLAIN: Asking for reasoning or why something is the case\n",
        "- PROCEDURE: Asking for how-to or steps\n",
        "- ADVICE: Asking for personalized or practical suggestions\n",
        "- COMPARISON: Asking to compare options\n",
        "- GENERAL: Anything else\n",
        "Return only the intent, nothing else.\n",
        "Here are a few examples:\"\"\",\n",
        "        ),\n",
        "        # few shot examples\n",
        "        few_shot_examples,\n",
        "        # New question\n",
        "        (\"user\", \"{question}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "GBnXsubRv18b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) Set up routing"
      ],
      "metadata": {
        "id": "_JfCqbXOv4Z8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "router = HuggingFaceChatRunnable(client, intent_prompt, 0.0, 10)"
      ],
      "metadata": {
        "id": "p9TlIiEfv81B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 7: Stepback Generation"
      ],
      "metadata": {
        "id": "ENG9vtZnv-4T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Step-back prompt"
      ],
      "metadata": {
        "id": "P4TBX0-YwDXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This are examples that shows the LLM what it is achieving through stepback\n",
        "\n",
        "examples = [\n",
        "    {\n",
        "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
        "        \"output\": \"what can the members of The Police do?\",\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
        "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
        "    },\n",
        "]\n",
        "\n",
        "# Now translate this into an example_prompt\n",
        "example_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"ai\", \"{output}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "few_shot = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples,\n",
        ")\n",
        "\n",
        "step_back_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
        "        ),\n",
        "        # Few shot examples\n",
        "        few_shot,\n",
        "        # New question\n",
        "        (\"user\", \"Intent: {intent}\\nQuestion: {question}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "IQCHdmF1wFFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) Set up Step Back"
      ],
      "metadata": {
        "id": "YzY7ii2rwHJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stepback = HuggingFaceChatRunnable(client, step_back_prompt, 0.0, 1024)"
      ],
      "metadata": {
        "id": "sgzx6DIqwJOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 8: Routing Chains"
      ],
      "metadata": {
        "id": "cpIlgBJewL5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Chains"
      ],
      "metadata": {
        "id": "9RVe_Wq9wPWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define prompt\n",
        "defineprompt = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
        "You are responding to a query with the intent: DEFINE.\n",
        "Your answer should be:\n",
        "- Comprehensive, but concise (1–3 sentences max)\n",
        "- Factually correct and aligned with the provided context\n",
        "- Free of speculation, advice, or subjective judgment\n",
        "- Focused only on essential information—no unnecessary background or examples unless they resolve ambiguity\n",
        "- Adjusted for multiple meanings if applicable\n",
        "- Written in terminology appropriate to the user's domain or field\n",
        "\n",
        "\n",
        "# {normal_context}\n",
        "# {step_back_context}\n",
        "\n",
        "# Original Question: {question}\n",
        "# Answer:\"\"\"\n",
        "\n",
        "#explain prompt\n",
        "explainprompt = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
        "You are responding to a query with the intent: EXPLAIN.\n",
        "Your answer should be:\n",
        "- Clear and logically structured\n",
        "- Focused on cause, reasoning, background, or significance\n",
        "- Factually correct and aligned with the provided context\n",
        "- Neutral in tone—avoid persuasion, speculation, or personal opinions\n",
        "- Examples are welcome from the context provided, if it helps to improve understanding.\n",
        "\n",
        "# {normal_context}\n",
        "# {step_back_context}\n",
        "\n",
        "# Original Question: {question}\n",
        "# Answer:\"\"\"\n",
        "\n",
        "#procedure prompt\n",
        "procedureprompt = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
        "You are responding to a query with the intent: PROCEDURE.\n",
        "Your answer should be:\n",
        "- Structured as a clear, ordered list of steps (e.g., 1, 2, 3...)\n",
        "- Focused on how-to instructions or best-practice sequences\n",
        "- Specific, practical, and applicable to the user’s likely context\n",
        "- Factually accurate and based on reliable knowledge\n",
        "- Aligned with the provided context; ignore context if irrelevant\n",
        "\n",
        "# {normal_context}\n",
        "# {step_back_context}\n",
        "\n",
        "# Original Question: {question}\n",
        "# Answer:\n",
        "1.\n",
        "2.\n",
        "3.\"\"\"\n",
        "\n",
        "#advice prompt\n",
        "adviceprompt = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
        "You are responding to a query with the intent: ADVICE.\n",
        "Your answer should be:\n",
        "- Actionable and practical, tailored to a general user (not personalized)\n",
        "- Fact-based, but sensitive to nuance, caution, or best practices\n",
        "- Free from subjective judgment or emotional language\n",
        "- Respectful of varying conditions or assumptions\n",
        "- Aligned with the provided context; if not relevant, ignore the context\n",
        "\n",
        "# {normal_context}\n",
        "# {step_back_context}\n",
        "\n",
        "# Original Question: {question}\n",
        "# Answer:\"\"\"\n",
        "\n",
        "#comparison\n",
        "comparisonprompt = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
        "You are responding to a query with the intent: COMPARISON.\n",
        "Your answer should be:\n",
        "- A neutral, side-by-side analysis of options or alternatives\n",
        "- Factually grounded—avoid personal recommendations unless one option is clearly superior based on evidence\n",
        "- Clearly structured with bullet points or short paragraphs\n",
        "- Helpful in illustrating pros and cons, similarities, and differences\n",
        "- Consistent with the context provided; ignore it if irrelevant\n",
        "\n",
        "# {normal_context}\n",
        "# {step_back_context}\n",
        "\n",
        "# Original Question: {question}\n",
        "# Answer:\n",
        "Option A:\n",
        "Option B: \"\"\"\n",
        "\n",
        "#general prompt\n",
        "generalprompt = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
        "You are responding to a query with the intent: GENERAL.\n",
        "Your answer should be:\n",
        "- Informative and contextually aware\n",
        "- Concise but flexible in length (aim for clarity)\n",
        "- Objective and based on verifiable information\n",
        "- Avoid speculation or personal opinion\n",
        "- Aligned with the provided context if relevant\n",
        "\n",
        "# {normal_context}\n",
        "# {step_back_context}\n",
        "\n",
        "# Original Question: {question}\n",
        "# Answer:\"\"\"\n"
      ],
      "metadata": {
        "id": "91k9M-kswSLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Load Prompt into a Global Variable"
      ],
      "metadata": {
        "id": "BbIXbQ-ZwUOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "intent_router = RunnableLambda(lambda x: {\n",
        "    \"DEFINE\": HuggingFaceChatRunnable(client, defineprompt, 0.0, 1024),\n",
        "    \"EXPLAIN\": HuggingFaceChatRunnable(client, explainprompt, 0.0, 1024),\n",
        "    \"PROCEDURE\": HuggingFaceChatRunnable(client, procedureprompt, 0.0, 1024),\n",
        "    \"ADVICE\": HuggingFaceChatRunnable(client, adviceprompt, 0.0, 1024),\n",
        "    \"COMPARISON\": HuggingFaceChatRunnable(client, comparisonprompt, 0.0, 1024),\n",
        "    \"GENERAL\": HuggingFaceChatRunnable(client, generalprompt, 0.0, 1024),\n",
        "}[x[\"intent\"].strip()]\n",
        ")"
      ],
      "metadata": {
        "id": "bLb3GKjOwa1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) Chain"
      ],
      "metadata": {
        "id": "ZKBeQCXiwbju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableMap, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "chain = (\n",
        "    RunnableMap({\n",
        "        \"question\": lambda x: x[\"question\"],\n",
        "        \"step_back_question\": lambda x: x[\"question\"],\n",
        "        \"intent\": lambda x: router.invoke({\"question\": x[\"question\"]})\n",
        "    })\n",
        "    | RunnableLambda(lambda x: {\n",
        "        \"normal_context\": retriever.invoke(x[\"question\"]),\n",
        "        \"step_back_q\": stepback.invoke({\"intent\" : x[\"intent\"],\"question\": x[\"step_back_question\"]}),\n",
        "        \"question\": x[\"question\"],\n",
        "        \"intent\": next(iter(x[\"intent\"])) if isinstance(x[\"intent\"], set) else x[\"intent\"]\n",
        "    })\n",
        "    | RunnableLambda(lambda x: {\n",
        "        \"step_back_context\": retriever.invoke(x[\"step_back_q\"]),\n",
        "        \"normal_context\": x[\"normal_context\"],\n",
        "        \"question\": x[\"question\"],\n",
        "        \"intent\": x[\"intent\"]\n",
        "    })\n",
        "    | intent_router\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "n8Eq0QRdwgWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 9: Running the Query"
      ],
      "metadata": {
        "id": "ALUkWQVmwq9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = input(\"Ask me anything! \\n\")\n",
        "\n",
        "# Generate the Response\n",
        "response = chain.invoke({\"question\": question})\n",
        "print(response)"
      ],
      "metadata": {
        "id": "FcO3Xt6hwvVa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}