{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1pqtO0cqSVlmYY1tVacBTFj4gNFGZDgQ1",
      "authorship_tag": "ABX9TyNm15vrzQ7MO61o2hz+0wwT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChowchowWorks/Customer_service_rag/blob/main/Rag_Pipeline_Prototype_Version_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: Importing Libraries"
      ],
      "metadata": {
        "id": "faXUqR8c98YY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Environment Toggle"
      ],
      "metadata": {
        "id": "kdChfPk89_mN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Lpwrl9MyuGMa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "!pip install langchain_community langchain chromadb transformers sentence-transformers\n",
        "!pip install -U langchain-huggingface\n",
        "\n",
        "!pip install pypdf\n",
        "\n",
        "os.environ['LANGCHAIN_API_KEY'] = \"API_KEY\"\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = \"API_KEY\"\n",
        "os.environ['USER_AGENT'] = 'MyColabApp/1.0 (Python/3.9; GoogleColab)'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Libraries"
      ],
      "metadata": {
        "id": "nLoBoa9S-Eg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.llms import HuggingFaceHub\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0xwqFRAZ92re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2: Load Documents"
      ],
      "metadata": {
        "id": "VDreIq3P-Me_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Load the Documents into a Global Variable"
      ],
      "metadata": {
        "id": "WpwmqBTs-O9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For the purpose of testing, this version uses a pdf loader\n",
        "loader = PyPDFDirectoryLoader(\"/content/RAG tester\")\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "cYCAch7K-TUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Splitting the document into more manageable chunks"
      ],
      "metadata": {
        "id": "w859HFgWBZfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size = 300, chunk_overlap = 50)\n",
        "texts = splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "X3blTDmoBeF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3: Indexing"
      ],
      "metadata": {
        "id": "2y8h1jDhBl6p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Embedding text chunks into the vector store"
      ],
      "metadata": {
        "id": "PT7OuvrOBoq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = Chroma.from_documents(texts, embedding)"
      ],
      "metadata": {
        "id": "FtXdihFnBoT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Declaring the number of chunks required to generate response"
      ],
      "metadata": {
        "id": "JCw0BsZNB8Wr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 5\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})"
      ],
      "metadata": {
        "id": "aoHG-IQfCBxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4: Implementing Step-Back Generator"
      ],
      "metadata": {
        "id": "P9cZ8SEECfX4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Chatbot"
      ],
      "metadata": {
        "id": "oS2BeW3YClg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "client = InferenceClient(model=\"mistralai/Mistral-7B-Instruct-v0.3\", token= \"API_KEY\")\n",
        "\n",
        "from langchain_core.runnables import Runnable\n",
        "\n",
        "class HuggingFaceChatRunnable(Runnable):\n",
        "    def __init__(self, client, prompt_template, temperature, max_tokens):\n",
        "        self.client = client\n",
        "        self.prompt_template = prompt_template\n",
        "        self.temperature = temperature\n",
        "        self.max_tokens = max_tokens\n",
        "\n",
        "    def invoke(self, inputs: dict, config: dict = None) -> str:\n",
        "        prompt_str = self.prompt_template.format(**inputs)\n",
        "\n",
        "        response = self.client.chat_completion(\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt_str}\n",
        "            ],\n",
        "            temperature=self.temperature,\n",
        "            max_tokens=self.max_tokens\n",
        "        )\n",
        "        return response.choices[0].message[\"content\"]"
      ],
      "metadata": {
        "id": "mPJyThXnDavn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Step Back Prompt"
      ],
      "metadata": {
        "id": "EFz_BS51DYqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import FewShotChatMessagePromptTemplate\n",
        "\n",
        "# This are examples that shows the LLM what it is achieving through stepback\n",
        "examples = [\n",
        "    {\n",
        "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
        "        \"output\": \"what can the members of The Police do?\",\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
        "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
        "    },\n",
        "]\n",
        "\n",
        "# Now translate this into an example_prompt\n",
        "example_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"ai\", \"{output}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "few_shot = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples,\n",
        ")\n",
        "\n",
        "step_back_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
        "        ),\n",
        "        # Few shot examples\n",
        "        few_shot,\n",
        "        # New question\n",
        "        (\"user\", \"{question}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "bEfXDJQxDvmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) Engineering the response prompt"
      ],
      "metadata": {
        "id": "IB_Jp0gfFavH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
        "\n",
        "# {normal_context}\n",
        "# {step_back_context}\n",
        "\n",
        "# Original Question: {question}\n",
        "# Answer:\"\"\"\n",
        "response_prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "ihFajls2FohO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(d) Implement the dual LLM"
      ],
      "metadata": {
        "id": "obQ8NwDFF8pq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# stepback generation\n",
        "stepback = HuggingFaceChatRunnable(client, prompt_template=step_back_prompt, temperature= 0, max_tokens= 1024)\n",
        "\n",
        "# Response generation\n",
        "chat = HuggingFaceChatRunnable(client, prompt_template= response_prompt, temperature = 0, max_tokens= 1024)"
      ],
      "metadata": {
        "id": "YBtrsVR_GAyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(e) Buidling the response chain"
      ],
      "metadata": {
        "id": "vsBljmkTGU9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableMap\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Chain for full RAG pipeline with step-back\n",
        "chain = (\n",
        "    RunnableMap({\n",
        "        \"question\": lambda x: x[\"question\"],\n",
        "        \"step_back_question\": lambda x: x[\"question\"]\n",
        "    })\n",
        "    | RunnableLambda(lambda x: {\n",
        "        \"normal_context\": retriever.invoke(x[\"question\"]),\n",
        "        \"step_back_q\": stepback.invoke({\"question\": x[\"step_back_question\"]}),\n",
        "        \"question\": x[\"question\"]\n",
        "    })\n",
        "    | RunnableLambda(lambda x: {\n",
        "        \"step_back_context\": retriever.invoke(x[\"step_back_q\"]),\n",
        "        \"normal_context\": x[\"normal_context\"],\n",
        "        \"question\": x[\"question\"]\n",
        "    })\n",
        "    | chat\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "vYiMJaQdGaWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 5: Running the Query"
      ],
      "metadata": {
        "id": "ZyP97Hz0IWy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "# Start with the question\n",
        "\n",
        "question = input(\"Ask me anything! \\n\")\n",
        "\n",
        "# Generate the Response\n",
        "response = chain.invoke({\"question\": question})\n",
        "\n",
        "wrapped_output = textwrap.fill(response, width=150)\n",
        "print(wrapped_output)"
      ],
      "metadata": {
        "id": "TPkV2wf0IWVi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}