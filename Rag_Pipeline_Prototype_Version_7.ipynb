{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPn2g2foXygsuD+TYh2VfaC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChowchowWorks/PN_RagPipeline/blob/main/Rag_Pipeline_Prototype_Version_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: Setting up Environment"
      ],
      "metadata": {
        "id": "EFG_9GUIJ-Sd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8hOFXKqO7LUN"
      },
      "outputs": [],
      "source": [
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "import os\n",
        "\n",
        "!pip install \"transformers==4.49.0\"\n",
        "!pip install -U langchain langchain-community langchainhub langchain-huggingface tiktoken langchain-cohere\n",
        "!pip install -U chromadb sentence-transformers\n",
        "!pip install -U pypdf\n",
        "!pip install -U ragatouille\n",
        "\n",
        "\n",
        "os.environ['LANGCHAIN_API_KEY'] = \"API_KEY\"\n",
        "os.environ['LANGSMITH_API_KEY'] = \"API_KEY\"\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = \"API_KEY\"\n",
        "os.environ['CO_API_KEY'] = \"API_KEY\"\n",
        "os.environ['USER_AGENT'] = 'MyColabApp/1.0 (Python/3.9; GoogleColab)'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2: Loading Documents"
      ],
      "metadata": {
        "id": "YMQdwzchJ7P7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "\n",
        "# For the purpose of testing, this version uses a pdf loader\n",
        "loader = PyPDFDirectoryLoader(\"/content/RAG tester\")\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "GLhlMo5cKGqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3: Indexing using ColBERT"
      ],
      "metadata": {
        "id": "9kqtkfR2KHtY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Setting Up RAGatoullie and Index"
      ],
      "metadata": {
        "id": "CvjhUB4EKSC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ragatouille import RAGPretrainedModel\n",
        "\n",
        "# Set up the indexing model\n",
        "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "\n",
        "# Index the documents\n",
        "RAG.index(collection = [doc.page_content for doc in documents], index_name = \"Behavioral\", max_document_length= 180, split_documents= True )"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1jMk4encKRbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Set up the retrieval process"
      ],
      "metadata": {
        "id": "mfGojftwKgQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = RAG.as_langchain_retriever(index_name = \"Behavioral\", k = 5)"
      ],
      "metadata": {
        "id": "dYXnbuNZKjJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4: Set up the Generator"
      ],
      "metadata": {
        "id": "5uVq1ykaKsEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "client = InferenceClient(model=\"mistralai/Mistral-7B-Instruct-v0.3\", token = os.environ['HUGGINGFACEHUB_API_TOKEN'])\n",
        "\n",
        "from langchain_core.runnables import Runnable\n",
        "\n",
        "class HuggingFaceChatRunnable(Runnable):\n",
        "    def __init__(self, client, prompt_template, temperature, max_tokens):\n",
        "        self.client = client\n",
        "        self.prompt_template = prompt_template\n",
        "        self.temperature = temperature\n",
        "        self.max_tokens = max_tokens\n",
        "\n",
        "    def invoke(self, inputs: dict, config: dict = None) -> str:\n",
        "        prompt_str = self.prompt_template.format(**inputs)\n",
        "\n",
        "        response = self.client.chat_completion(\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt_str}\n",
        "            ],\n",
        "            temperature=self.temperature,\n",
        "            max_tokens=self.max_tokens\n",
        "        )\n",
        "        return response.choices[0].message[\"content\"]"
      ],
      "metadata": {
        "id": "ynCUAz87Kuxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 5: Query Analysis (Routing)"
      ],
      "metadata": {
        "id": "sm-OfUS1K4m1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "system = \"You are an expert at routing user questions to a vectorstore or a Command R chatbot. The vectorstore contains documents related to behavioral economics, behavioral sciences and psychology by Daniel Kahneman, Richard Thaler and Cass R Sunstein. Use the vectorstore for questions related to these topics, otherwise route the question to the Command R chatbot. Your answer should be literally be either CommandR or vectorstore. Do not output anything else.\"\n",
        "route_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "analysis_router = HuggingFaceChatRunnable(client, route_prompt, 0, 5)"
      ],
      "metadata": {
        "id": "YPUk5JUIK9-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 6: Grading Documents"
      ],
      "metadata": {
        "id": "t2gHiTE2cauP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grading = \"You are an expert at determining whether a document is related to a user question. If the document contains keyword(s) or contain semantic meaning that is related to the user question, grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. Do not output anything else.\"\n",
        "\n",
        "grade_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", grading),\n",
        "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "grader = HuggingFaceChatRunnable(client, grade_prompt,0, 5)\n",
        "\n",
        "#question = \"Policy making\"\n",
        "#doc1 = retriever.invoke(question)\n",
        "# print(grader.invoke({\"document\": doc1[2], \"question\": question}))"
      ],
      "metadata": {
        "id": "eXI-E1ZWcl-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 7: Intent Detection"
      ],
      "metadata": {
        "id": "lLbjSssQf6Bw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Fewshot examples"
      ],
      "metadata": {
        "id": "1YXD6axrgB4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.prompts import FewShotChatMessagePromptTemplate\n",
        "\n",
        "# Some fewshot examples\n",
        "examples =[\n",
        "    {\n",
        "        \"input\": \"What is creatine?\",\n",
        "        \"output\": \"DEFINE\",\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Why do athletes take protein after workouts?\",\n",
        "        \"output\": \"EXPLAIN\",\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"How do I calculate my calorie needs?\",\n",
        "        \"output\": \"PROCEDURE\",\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Should I take whey or casein protein?\",\n",
        "        \"output\": \"COMPARISON\",\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"What is the best way to embark on my weight loss journey?\",\n",
        "        \"output\": \"ADVICE\",\n",
        "    },\n",
        "]\n",
        "\n",
        "\n",
        "example_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"ai\", \"{output}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "few_shot_examples = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples,\n",
        ")"
      ],
      "metadata": {
        "id": "lk-b9MHfgEIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Intent detection prompt"
      ],
      "metadata": {
        "id": "_8w04cHjgHVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "intent_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"You are an intent classifier for the field of interest in the query.\n",
        "Given a question, classify it into one of the following intents:\n",
        "- DEFINE: Asking for a definition or description\n",
        "- EXPLAIN: Asking for reasoning or why something is the case\n",
        "- PROCEDURE: Asking for how-to or steps\n",
        "- ADVICE: Asking for personalized or practical suggestions\n",
        "- COMPARISON: Asking to compare options\n",
        "- GENERAL: Anything else\n",
        "Return only the intent, nothing else.\n",
        "Here are a few examples:\"\"\",\n",
        "        ),\n",
        "        # few shot examples\n",
        "        few_shot_examples,\n",
        "        # New question\n",
        "        (\"user\", \"{question}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "JdDpvUy8gJJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) intent detecter"
      ],
      "metadata": {
        "id": "XRIsRADUgNqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "intent_router = HuggingFaceChatRunnable(client, intent_prompt, 0.0, 10)"
      ],
      "metadata": {
        "id": "HpV3qvosgQ1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 8: Step-back Translation"
      ],
      "metadata": {
        "id": "DtaqmKFniIRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This are examples that shows the LLM what it is achieving through stepback\n",
        "\n",
        "examples = [\n",
        "    {\n",
        "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
        "        \"output\": \"what can the members of The Police do?\",\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
        "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
        "    },\n",
        "]\n",
        "\n",
        "# Now translate this into an example_prompt\n",
        "example_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"ai\", \"{output}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "few_shot = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples,\n",
        ")\n",
        "\n",
        "step_back_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Take into consideration the intent of the user and their objective of asking the question. Here are a few examples:\"\"\",\n",
        "        ),\n",
        "        # Few shot examples\n",
        "        few_shot,\n",
        "        # New question\n",
        "        (\"user\", \"Intent: {intent}\\nQuestion: {question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "stepback = HuggingFaceChatRunnable(client, step_back_prompt, 0.0, 1024)"
      ],
      "metadata": {
        "id": "h1QC4GMLiH0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 9: Generating an Output"
      ],
      "metadata": {
        "id": "61yNG6iKgaTp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Intent routing"
      ],
      "metadata": {
        "id": "rBpJj4pRglS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define prompt\n",
        "defineprompt = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Answer the question using only the information in the provided context blocks.\n",
        "You are responding to a query with the intent: DEFINE.\n",
        "Your answer should be:\n",
        "- Use the retrieved context as your only source of truth\n",
        "- Do not rely on external or prior knowledge, even if you think it’s correct\n",
        "- If the context does not contain enough information, say \"The information is not available in the context provided.\"\n",
        "- Comprehensive, but concise (1–3 sentences max)\n",
        "- Factually correct and aligned with the provided context\n",
        "- Free of speculation, advice, or subjective judgment\n",
        "- Focused only on essential information—no unnecessary background or examples unless they resolve ambiguity\n",
        "- Adjusted for multiple meanings if applicable\n",
        "- Written in terminology appropriate to the user's domain or field\n",
        "\n",
        "\n",
        "# {normal_context}\n",
        "# {step_back_context}\n",
        "\n",
        "# Original Question: {question}\n",
        "# Answer:\"\"\"\n",
        "\n",
        "#explain prompt\n",
        "explainprompt = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Answer the question using only the information in the provided context blocks.\n",
        "You are responding to a query with the intent: EXPLAIN.\n",
        "Your answer should be:\n",
        "- Use the retrieved context as your only source of truth\n",
        "- Do not rely on external or prior knowledge, even if you think it’s correct\n",
        "- If the context does not contain enough information, say \"The information is not available in the context provided.\"\n",
        "- Clear and logically structured\n",
        "- Focused on cause, reasoning, background, or significance\n",
        "- Factually correct and aligned with the provided context\n",
        "- Neutral in tone—avoid persuasion, speculation, or personal opinions\n",
        "- Examples are welcome from the context provided, if it helps to improve understanding.\n",
        "\n",
        "# {normal_context}\n",
        "# {step_back_context}\n",
        "\n",
        "# Original Question: {question}\n",
        "# Answer:\"\"\"\n",
        "\n",
        "#procedure prompt\n",
        "procedureprompt = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Answer the question using only the information in the provided context blocks.\n",
        "You are responding to a query with the intent: PROCEDURE.\n",
        "Your answer should be:\n",
        "- Use the retrieved context as your only source of truth\n",
        "- Do not rely on external or prior knowledge, even if you think it’s correct\n",
        "- If the context does not contain enough information, say \"The information is not available in the context provided.\"\n",
        "- Structured as a clear, ordered list of steps (e.g., 1, 2, 3...)\n",
        "- Focused on how-to instructions or best-practice sequences\n",
        "- Specific, practical, and applicable to the user’s likely context\n",
        "- Factually accurate and based on reliable knowledge\n",
        "- Aligned with the provided context; ignore context if irrelevant\n",
        "\n",
        "# {normal_context}\n",
        "# {step_back_context}\n",
        "\n",
        "# Original Question: {question}\n",
        "# Answer:\n",
        "1.\n",
        "2.\n",
        "3.\"\"\"\n",
        "\n",
        "#advice prompt\n",
        "adviceprompt = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Answer the question using only the information in the provided context blocks.\n",
        "You are responding to a query with the intent: ADVICE.\n",
        "Your answer should be:\n",
        "- Use the retrieved context as your only source of truth\n",
        "- Do not rely on external or prior knowledge, even if you think it’s correct\n",
        "- If the context does not contain enough information, say \"The information is not available in the context provided.\"\n",
        "- Actionable and practical, tailored to a general user (not personalized)\n",
        "- Fact-based, but sensitive to nuance, caution, or best practices\n",
        "- Free from subjective judgment or emotional language\n",
        "- Respectful of varying conditions or assumptions\n",
        "- Aligned with the provided context; if not relevant, ignore the context\n",
        "\n",
        "# {normal_context}\n",
        "# {step_back_context}\n",
        "\n",
        "# Original Question: {question}\n",
        "# Answer:\"\"\"\n",
        "\n",
        "#comparison\n",
        "comparisonprompt = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Answer the question using only the information in the provided context blocks.\n",
        "You are responding to a query with the intent: COMPARISON.\n",
        "Your answer should be:\n",
        "- Use the retrieved context as your only source of truth\n",
        "- Do not rely on external or prior knowledge, even if you think it’s correct\n",
        "- If the context does not contain enough information, say \"The information is not available in the context provided.\"\n",
        "- A neutral, side-by-side analysis of options or alternatives\n",
        "- Factually grounded—avoid personal recommendations unless one option is clearly superior based on evidence\n",
        "- Clearly structured with bullet points or short paragraphs\n",
        "- Helpful in illustrating pros and cons, similarities, and differences\n",
        "- Consistent with the context provided; ignore it if irrelevant\n",
        "\n",
        "# {normal_context}\n",
        "# {step_back_context}\n",
        "\n",
        "# Original Question: {question}\n",
        "# Answer:\n",
        "Option A:\n",
        "Option B: \"\"\"\n",
        "\n",
        "#general prompt\n",
        "generalprompt = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Answer the question using only the information in the provided context blocks.\n",
        "You are responding to a query with the intent: GENERAL.\n",
        "Your answer should be:\n",
        "- Use the retrieved context as your only source of truth\n",
        "- Do not rely on external or prior knowledge, even if you think it’s correct\n",
        "- If the context does not contain enough information, say \"The information is not available in the context provided.\"\n",
        "- Informative and contextually aware\n",
        "- Concise but flexible in length (aim for clarity)\n",
        "- Objective and based on verifiable information\n",
        "- Avoid speculation or personal opinion\n",
        "- Aligned with the provided context if relevant\n",
        "\n",
        "# {normal_context}\n",
        "# {step_back_context}\n",
        "\n",
        "# Original Question: {question}\n",
        "# Answer:\"\"\"\n"
      ],
      "metadata": {
        "id": "wVpCQJNign6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "generator = RunnableLambda(lambda x: {\n",
        "    \"DEFINE\": HuggingFaceChatRunnable(client, defineprompt, 0.0, 1024),\n",
        "    \"EXPLAIN\": HuggingFaceChatRunnable(client, explainprompt, 0.0, 1024),\n",
        "    \"PROCEDURE\": HuggingFaceChatRunnable(client, procedureprompt, 0.0, 1024),\n",
        "    \"ADVICE\": HuggingFaceChatRunnable(client, adviceprompt, 0.0, 1024),\n",
        "    \"COMPARISON\": HuggingFaceChatRunnable(client, comparisonprompt, 0.0, 1024),\n",
        "    \"GENERAL\": HuggingFaceChatRunnable(client, generalprompt, 0.0, 1024),\n",
        "}[x[\"intent\"].strip()]\n",
        ")"
      ],
      "metadata": {
        "id": "g6h7QpIxghGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 10: Hallucination Detection"
      ],
      "metadata": {
        "id": "h6pherPhhHBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hallucinate = \"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts. Do not output anything else.\"\n",
        "\n",
        "hallucinator_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", hallucinate),\n",
        "        (\"human\", \"Retrieved facts: \\n\\n {documents} \\n\\n LLM answer: {answer}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "hallucinator = HuggingFaceChatRunnable(client, hallucinator_prompt, 0, 5)\n"
      ],
      "metadata": {
        "id": "5Os6HZEhhPuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 11: Answer Grader"
      ],
      "metadata": {
        "id": "tP2f5r0enCU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answerings = \"You are a grader assessing whether an answer addresses / resolves a question. Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question. Do not output anything else.\"\n",
        "\n",
        "answering_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", answerings),\n",
        "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "answer_grader = HuggingFaceChatRunnable(client, answering_prompt, 0, 5)"
      ],
      "metadata": {
        "id": "_EnZcJj-nFVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 12: Question rewritter"
      ],
      "metadata": {
        "id": "E6u21Dj7AFv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rewrittings = \"You are a expert in framing questions to enable good retrieval from the ColBERT vector store. You will rewrite the given question such that it is able to retrieve more relevant documents from the vector store. If there is a follow up question asked and an answer provided by the user, incorporate the additional information into the rewritten question. Output only the question and do not output anything else. The rewritten question should be approximately the same length as the original question.\"\n",
        "\n",
        "rewritting_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", rewrittings),\n",
        "        (\"human\", \"Original question: \\n\\n {question} \\n\\n Follow-up Question: \\n\\n {follow_up} \\n\\n User response to follow-up Question: \\n\\n {info}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "rewritter = HuggingFaceChatRunnable(client, rewritting_prompt, 0, 1024)"
      ],
      "metadata": {
        "id": "jaTFTpN1AK6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 13: Reviewer"
      ],
      "metadata": {
        "id": "al6nKJgYQnQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviewings = \"You are an expert in query analysis for a RAG pipeline. Your task is to examine the question posed by the user and determine if there is enough information in the query to engage in meaningful retrieval. Give a binary score 'yes' or 'no'. Do not output anything else.\"\n",
        "\n",
        "reviweing_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", reviewings),\n",
        "        (\"human\", \"User question: \\n\\n {question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "reviewer = HuggingFaceChatRunnable(client, reviweing_prompt, 0, 5)\n",
        "\n",
        "askings = \"You are an expert at prompting additional information from users. Your task is to examine the question posed by the user and ask a follow up question that would make retrieval more meaningful. Output only the question and do not output anything else. The rewritten question should be approximately the same length as the original question.\"\n",
        "\n",
        "asking_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", askings),\n",
        "        (\"human\", \"User question: \\n\\n {question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "asker = HuggingFaceChatRunnable(client, asking_prompt, 0, 1024)"
      ],
      "metadata": {
        "id": "7kaB19xXQqxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 14: Helper Functions"
      ],
      "metadata": {
        "id": "RYLMO9Ozoj9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_cohere import ChatCohere\n",
        "\n",
        "def routing(question):\n",
        "  # pass the question into the router; get back vectorstore or CommandR\n",
        "  try:\n",
        "    route = analysis_router.invoke({\"question\": question})\n",
        "  except Exception as e:\n",
        "    print(\"---Error: Router failed to route query---\")\n",
        "    print(f\"Reason: {e}\")\n",
        "    route = None\n",
        "  print(f\"--- Routed to {route}---\")\n",
        "  if route.lower() == \"vectorstore\":\n",
        "    return True\n",
        "  elif route.lower() == \"CommandR\":\n",
        "    return False\n",
        "  else:\n",
        "    print(f\"---Error: Router routed to {route}---\")\n",
        "    return None\n",
        "\n",
        "def intentions(question):\n",
        "  print(\"---Determining Intent\")\n",
        "  try:\n",
        "    intent = intent_router.invoke({\"question\": question})\n",
        "    print(f\"---Intent identified as {intent}---\")\n",
        "  except Exception as e:\n",
        "    print(\"---Error: Intent detection failed---\")\n",
        "    print(f\"Reason: {e}\")\n",
        "    intent = None\n",
        "  return intent\n",
        "\n",
        "def CmdRchain(question):\n",
        "  CmdR = ChatCohere(model=\"command-r\", co_api_key=os.environ['CO_API_KEY'])\n",
        "  try:\n",
        "    response = CmdR.invoke(question)\n",
        "  except Exception as e:\n",
        "    print(\"---Error: CommandR failed to generate output---\")\n",
        "    print(f\"Reason: {e}\")\n",
        "    response = \"Error Code 1: Command R failed to generate output.\"\n",
        "  return response\n",
        "\n",
        "def stepback_translation(question, intent):\n",
        "  print(\"---Step-back Translation---\")\n",
        "  try:\n",
        "    translation = stepback.invoke({\"question\": question, \"intent\": intent})\n",
        "    print(f\"---Step-back Translation: {translation}\")\n",
        "  except Exception as e:\n",
        "    print(\"---Error: Step-back translation failed---\")\n",
        "    print(f\"Reason: {e}\")\n",
        "    translation = None\n",
        "  return translation\n",
        "\n",
        "def bad_grading(question):\n",
        "  print(\"---Determining if additional information is required.\")\n",
        "  try:\n",
        "    verdict = reviewer.invoke({'question': question})\n",
        "  except Exception as e:\n",
        "    print(\"---Error: Reviewer failed to determine if additional information is required---\")\n",
        "    print(f\"Reason: {e}\")\n",
        "    verdict = None\n",
        "  if verdict.lower() == \"no\":\n",
        "    return rewritter.invoke({'question': question, 'info': None})\n",
        "  elif verdict.lower() == \"yes\":\n",
        "    follow_up = asker.invoke({'question': question})\n",
        "    info = input(f\"{follow_up}\")\n",
        "    return rewritter.invoke({'question': question, 'follow_up': follow_up, 'info': info})\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "def retrieval(question, retries): # retrieve and evaluate retrieved documents\n",
        "  if retries > 3:\n",
        "    print(\"--- Error: Too many retries, ask another question!---\")\n",
        "    return None\n",
        "  try:\n",
        "    context = retriever.invoke(question)\n",
        "  except Exception as e:\n",
        "    print(\"---Error: Retriever failed to retrieve documents\")\n",
        "    print(f\"Reason: {e}\")\n",
        "    return None\n",
        "  print(\"---Grading documents---\")\n",
        "  try:\n",
        "    grade = grader.invoke({\"document\": context[2], \"question\": question})\n",
        "  except Exception as e:\n",
        "    print(\"---Error: Grader failed to grade---\")\n",
        "    print(f\"Reason: {e}\")\n",
        "    grade = None\n",
        "  if grade.lower() == \"yes\":\n",
        "    print(\"---Documents are relevant---\")\n",
        "    return context\n",
        "  elif grade.lower() == 'no':\n",
        "    print(\"---Documents are not relevant\")\n",
        "    print(\"---Rewritting Question---\")\n",
        "    new_question = bad_grading(question)\n",
        "    retries += 1\n",
        "    return retrieval(new_question, retries)\n",
        "  else:\n",
        "    print(f\"---Error: Grader graded documents as {grade}\")\n",
        "    return None\n",
        "\n",
        "def generation(question, normal_context, step_back_context, intent):\n",
        "  print(\"---Generating the output---\")\n",
        "  try:\n",
        "    output = generator.invoke({'question': question, 'normal_context':normal_context, \"step_back_context\": step_back_context, 'intent': intent})\n",
        "  except Exception as e:\n",
        "    print(\"---Error:Generator failed to generate output---\")\n",
        "    print(f\"Reason: {e}\")\n",
        "    output = None\n",
        "  return output\n",
        "\n",
        "def hallucation_detector(documents, answer):\n",
        "  print(\"---Detecting Hallucination---\")\n",
        "  try:\n",
        "    hallucinate = hallucinator.invoke({'documents': documents, 'answer': answer})\n",
        "  except Exception as e:\n",
        "    print(\"---Error: Hallucinator failed in detecting hallucination---\")\n",
        "    print(f\"Reason: {e}\")\n",
        "  if hallucinate.lower() == 'yes':\n",
        "    return True\n",
        "  elif hallucinate.lower() == 'no':\n",
        "    return False\n",
        "  else:\n",
        "    print(f\"---Error: Hallucinator returned {hallucinate}\")\n",
        "    return None\n",
        "\n",
        "def accuracy_checker(question, answer):\n",
        "  print(\"---Checking OutputAccuracy---\")\n",
        "  try:\n",
        "    grade = answer_grader.invoke({'question':question, 'generation': answer})\n",
        "  except Exception as e:\n",
        "    print(\"---Error: Answer Grader failed to Grade answer\")\n",
        "    print(f\"Reason:{e}\")\n",
        "  if grade == 'yes':\n",
        "    return True\n",
        "  elif grade == 'no':\n",
        "    return False\n",
        "  else:\n",
        "    print(f\"---Error: Answer Grader returned {grade}\")\n",
        "    return None\n",
        "\n",
        "def vectoring(question):\n",
        "  # intent detection step\n",
        "  intent = intentions(question)\n",
        "  if intent:\n",
        "  # step back translation\n",
        "    step_back_question = stepback_translation(question, intent)\n",
        "    if step_back_question == None:\n",
        "      raise Exception(\"Error code 2b: Failed to generate stepback question\")\n",
        "  elif intent == None:\n",
        "    raise Exception(\"Error code 2a: Intent detection failed\")\n",
        "  # get normal context and stepback context\n",
        "  normal_context = retrieval(question, 0)\n",
        "  if normal_context == None:\n",
        "    raise Exception(\"Error code 2c: Failed to retreive normal context\")\n",
        "  step_back_context = retrieval(step_back_question, 0)\n",
        "  if step_back_context == None:\n",
        "    raise Exception(\"Error code 2d: Failed to retreive stepback context\")\n",
        "  # Generate the output and validate\n",
        "  max_retries, retries = 3, 0\n",
        "  while retries <= max_retries:\n",
        "    output = generation(question, normal_context, step_back_context, intent)\n",
        "    if output == None:\n",
        "      raise Exception(\"Error code 2f: Failed to generate output\")\n",
        "    hallucinate = hallucation_detector(normal_context + step_back_context, output)\n",
        "    if hallucinate == None:\n",
        "      raise Exception(\"Error code 2e: Failed to run hallucination detector\")\n",
        "    if hallucinate == 'no':\n",
        "      break\n",
        "    else:\n",
        "      retries += 1\n",
        "  return output\n",
        "\n",
        "def redo(question, output):\n",
        "  if accuracy_checker(question, output):\n",
        "    return output\n",
        "  elif not accuracy_checker:\n",
        "    return vectoring(question)\n",
        "  elif accuracy_checker == None:\n",
        "    raise Exception(\"Error code 2g: Accuracy checker failed\")\n",
        "\n",
        "\n",
        "def initializer(question):\n",
        "  route = routing(question)\n",
        "  if not route:\n",
        "    return CmdRchain(question)\n",
        "  elif route:\n",
        "    return(redo(question, vectoring(question)))\n",
        "  elif route == None:\n",
        "    raise Exception(\"Error code 2h: Routing failed\")"
      ],
      "metadata": {
        "id": "PixAimg8e2hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 15: Run Everything"
      ],
      "metadata": {
        "id": "Mh01IWFmvmHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = input(\"Enter your question: \")\n",
        "print(initializer(question))"
      ],
      "metadata": {
        "id": "fJtj3JGPvoa5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}