{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPMBdnhsHvHhUlAz+8uArL1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChowchowWorks/Customer_service_rag/blob/main/Rag_Pipeline_Prototype_Version_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: Import Libraries"
      ],
      "metadata": {
        "id": "X18Twq6y8CDR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2Zgs6CM6GSS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "!pip install langchain_community langchain chromadb transformers sentence-transformers\n",
        "!pip install -U langchain-huggingface\n",
        "!pip install wikipedia\n",
        "!pip install pypdf\n",
        "\n",
        "os.environ['LANGCHAIN_API_KEY'] = \"API_KEY\"\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = \"API_KEY\"\n",
        "os.environ['USER_AGENT'] = 'MyColabApp/1.0 (Python/3.9; GoogleColab)'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.document_loaders import WikipediaLoader\n",
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.llms import HuggingFaceHub\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dxpG2fyy8d5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2: Load Documents"
      ],
      "metadata": {
        "id": "rhX4Re7Y8l0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Load the documents into a global variable"
      ],
      "metadata": {
        "id": "YalpjJTW88ny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For the purposes of testing, this code uses a Wikipedia page as a document\n",
        "loader = WikipediaLoader(query=\"National University of Singapore\", lang = 'en')\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "wSZOb1cO8pz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Split the documents into more manageable chunks"
      ],
      "metadata": {
        "id": "HqKP5Sg49Anf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size = 300, chunk_overlap = 50)\n",
        "texts = splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "jnMXQLMK9Ens"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3: Indexing"
      ],
      "metadata": {
        "id": "uhjjsv7r9LZz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Initialise the pipeline by indexing the documents"
      ],
      "metadata": {
        "id": "OHumkBjw9QL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = Chroma.from_documents(texts, embedding)"
      ],
      "metadata": {
        "id": "TLeJvG_39Pwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Choose the number of similar documents to be retrieved from the pipeline"
      ],
      "metadata": {
        "id": "LbqNhIyP9aDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 3\n",
        "retriever = vectorstore.as_retriever(search_kwargs = {'k' : k})"
      ],
      "metadata": {
        "id": "qUKAgxUE9ZsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4: Implementing the Generator"
      ],
      "metadata": {
        "id": "h6kMLznW9hHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Defining the prompt template"
      ],
      "metadata": {
        "id": "TMp1wziT9svO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "QWCQqr8U9sQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Huggingface Inferface Client"
      ],
      "metadata": {
        "id": "-3xKCCKk9zaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "client = InferenceClient(model=\"mistralai/Mistral-7B-Instruct-v0.3\", token= \"API_KEY\")"
      ],
      "metadata": {
        "id": "b5YimYsO933P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) Runnable Class and Initialisation"
      ],
      "metadata": {
        "id": "niJBIJtX956E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import Runnable\n",
        "\n",
        "class HuggingFaceChatRunnable(Runnable):\n",
        "    def __init__(self, client, prompt_template, temperature, max_tokens):\n",
        "        self.client = client\n",
        "        self.prompt_template = prompt_template\n",
        "        self.temperature = temperature\n",
        "        self.max_tokens = max_tokens\n",
        "\n",
        "    def invoke(self, inputs: dict) -> str:\n",
        "        prompt_str = self.prompt_template.format(**inputs)\n",
        "\n",
        "        response = self.client.chat_completion(\n",
        "            messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt_str}],\n",
        "            temperature = self.temperature,\n",
        "            max_tokens = self.max_tokens)\n",
        "        return response.choices[0].message[\"content\"]\n",
        "\n",
        "chat = HuggingFaceChatRunnable(client, prompt_template=prompt, temperature= 0.4, max_tokens= 1024)"
      ],
      "metadata": {
        "id": "7TvcBIA5-AxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 5: Retrieval and Response"
      ],
      "metadata": {
        "id": "omMybsOB-C1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Retrieval"
      ],
      "metadata": {
        "id": "PH3aGkss-Paz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from logging import exception\n",
        "# Start by asking the question\n",
        "question = input(\"Ask me anything!\\n\")\n",
        "# retreive the relevant documents\n",
        "relevant = retriever.invoke(question)\n",
        "# checks\n",
        "if len(relevant) != k:\n",
        "  raise exception(f\"Wrong number of relevant documents: expected {k}, got {len(relevant)}\")"
      ],
      "metadata": {
        "id": "ZsjV6jUR-J6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Response"
      ],
      "metadata": {
        "id": "7E0LPbEI-rVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.invoke({\"context\": relevant, \"question\": question})\n",
        "print(response)"
      ],
      "metadata": {
        "id": "XgubVr6v-tSA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}